{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3720f348",
   "metadata": {},
   "source": [
    "### Setup do Spark e Top 20 de tempos m√©dios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5cf8ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/05 16:41:44 WARN Utils: Your hostname, MacBook-Air-de-Kaleb.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.32 instead (on interface en0)\n",
      "25/10/05 16:41:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/05 16:41:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2446)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2446)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:339)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m os.environ[\u001b[33m'\u001b[39m\u001b[33mHADOOP_HOME\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mhadoop\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     10\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mhadoop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mbin\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAnaliseETL-DataViz\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlocal[*]\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mspark.driver.host\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m127.0.0.1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m base_path = \u001b[33m'\u001b[39m\u001b[33mC:/Users/julii/OneDrive/Documentos/formula1-analytics/Data Layer/raw/dados_originais\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     13\u001b[39m file_names = [\u001b[33m'\u001b[39m\u001b[33mcircuits\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mconstructors\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdrivers\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlap_times\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpit_stops\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mqualifying\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mraces\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msprint_results\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/formula1-analytics-2/.venv/lib/python3.11/site-packages/pyspark/sql/session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/formula1-analytics-2/.venv/lib/python3.11/site-packages/pyspark/core/context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/formula1-analytics-2/.venv/lib/python3.11/site-packages/pyspark/core/context.py:207\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    205\u001b[39m SparkContext._ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway=gateway, conf=conf)\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m.stop()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/formula1-analytics-2/.venv/lib/python3.11/site-packages/pyspark/core/context.py:300\u001b[39m, in \u001b[36mSparkContext._do_init\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28mself\u001b[39m.environment[\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m] = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m \u001b[38;5;28mself\u001b[39m._jsc = jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[32m    302\u001b[39m \u001b[38;5;28mself\u001b[39m._conf = SparkConf(_jconf=\u001b[38;5;28mself\u001b[39m._jsc.sc().conf())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/formula1-analytics-2/.venv/lib/python3.11/site-packages/pyspark/core/context.py:429\u001b[39m, in \u001b[36mSparkContext._initialize_context\u001b[39m\u001b[34m(self, jconf)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[33;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[32m    427\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/formula1-analytics-2/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1627\u001b[39m, in \u001b[36mJavaClass.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1621\u001b[39m command = proto.CONSTRUCTOR_COMMAND_NAME +\\\n\u001b[32m   1622\u001b[39m     \u001b[38;5;28mself\u001b[39m._command_header +\\\n\u001b[32m   1623\u001b[39m     args_command +\\\n\u001b[32m   1624\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1626\u001b[39m answer = \u001b[38;5;28mself\u001b[39m._gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1627\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/formula1-analytics-2/.venv/lib/python3.11/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is supported only if a security manager is allowed\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:347)\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2446)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2446)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:339)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:501)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:485)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1575)\n"
     ]
    }
   ],
   "source": [
    "# Configura√ß√£o de Ambiente\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, concat_ws\n",
    "os.environ['HADOOP_HOME'] = 'C:\\\\hadoop'\n",
    "sys.path.append('C:\\\\hadoop\\\\bin')\n",
    "spark = SparkSession.builder.appName('AnaliseETL-DataViz').master('local[*]').config('spark.driver.host', '127.0.0.1').getOrCreate()\n",
    "base_path = 'C:/Users/julii/OneDrive/Documentos/formula1-analytics/Data Layer/raw/dados_originais'\n",
    "file_names = ['circuits', 'constructors', 'drivers', 'lap_times', 'pit_stops', 'qualifying', 'races', 'results', 'status', 'sprint_results']\n",
    "dataframes = {}\n",
    "try:\n",
    "    for name in file_names:\n",
    "        file_path = f'{base_path}/{name}.csv'\n",
    "        dataframes[name] = spark.read.csv(file_path, header=True, inferSchema=True, nullValue='\\\\N')\n",
    "except Exception as e:\n",
    "    print(f'\\nERRO AO CARREGAR OS ARQUIVOS. Verifique o caminho base.')\n",
    "    spark.stop()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ea5f3",
   "metadata": {},
   "source": [
    "- **Objetivo do gr√°fico:** Visualizar o ranking dos 20 pilotos com menor tempo m√©dio de volta em toda a base hist√≥rica.\n",
    "- **Dados utilizados:** `lap_times` para tempos por volta e `drivers` para montar o nome completo de cada piloto.\n",
    "- **A√ß√µes principais:** Inicializa a SparkSession local; l√™ os CSVs cadastrados em `file_names` para o dicion√°rio `dataframes`; agrega o tempo m√©dio por piloto; converte o resultado para segundos e para Pandas; monta o gr√°fico de barras horizontal destacando os 20 melhores.\n",
    "- **Sa√≠da:** Figura `Tempo M√©dio de Volta por Piloto (Top 20)` em Matplotlib e impress√£o do piloto l√≠der em tempo m√©dio.\n",
    "- **Insight esperado:** Identificar quem mant√©m o ritmo mais r√°pido de forma consistente, base para compara√ß√µes entre pilotos.\n",
    "- **Observa√ß√µes t√©cnicas:** A configura√ß√£o do ambiente e o dicion√°rio `dataframes` s√£o reutilizados por todas as c√©lulas posteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077acd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_lap_time_df = dataframes['lap_times'].groupBy('driverId').agg(avg('milliseconds').alias('avg_milliseconds'))\n",
    "drivers_with_label = dataframes['drivers'].withColumn('driver_name', concat_ws(' ', col('forename'), col('surname')))\n",
    "avg_lap_time_with_driver = avg_lap_time_df.join(drivers_with_label, 'driverId').withColumn('avg_seconds', col('avg_milliseconds') / 1000).orderBy('avg_seconds')\n",
    "plot_data_2 = avg_lap_time_with_driver.select('driver_name', 'avg_seconds').toPandas()\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.barplot(x='avg_seconds', y='driver_name', data=plot_data_2.head(20), orient='h', palette='rocket_r', hue='driver_name', legend=False)\n",
    "plt.title('Tempo M√©dio de Volta por Piloto (Top 20)', fontsize=16)\n",
    "plt.xlabel('Tempo de Volta M√©dio (segundos)', fontsize=12)\n",
    "plt.ylabel('Piloto', fontsize=12)\n",
    "plt.tight_layout()\n",
    "fastest_driver = plot_data_2.iloc[0]\n",
    "print(f\"O piloto mais r√°pido em m√©dia √© {fastest_driver['driver_name']} com um tempo m√©dio de {fastest_driver['avg_seconds']:.3f} segundos.\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65344b5c",
   "metadata": {},
   "source": [
    "### Voltas recordes por corrida\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad9389f",
   "metadata": {},
   "source": [
    "- **Objetivo do gr√°fico:** Descobrir quais corridas registraram as voltas mais r√°pidas da hist√≥ria.\n",
    "- **Dados utilizados:** `lap_times` para calcular o menor tempo por corrida e `races` para compor o r√≥tulo nome + ano.\n",
    "- **A√ß√µes principais:** Agrupa as voltas por `raceId` pegando o tempo m√≠nimo; junta com os metadados da corrida; converte tempos para segundos; ordena da volta mais r√°pida para a mais lenta; cria o gr√°fico de barras horizontal com o Top 20.\n",
    "- **Sa√≠da:** Figura `Volta Mais R√°pida por Corrida (Top 20)` e mensagem no console apontando a prova com volta mais veloz.\n",
    "- **Insight esperado:** Evidenciar eventos com condi√ß√µes excepcionais de desempenho, √∫teis para an√°lises comparativas por temporada ou circuito.\n",
    "- **Complemento √∫til:** Permite correlacionar corridas r√°pidas com vari√°veis externas (clima, pneus, regulamento) em estudos adicionais.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c80a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, col, concat_ws\n",
    "fastest_lap_per_race_df = dataframes['lap_times'].groupBy('raceId').agg(min('milliseconds').alias('fastest_lap_ms'))\n",
    "races_with_label = dataframes['races'].withColumn('race_event_label', concat_ws(' ', col('name'), col('year')))\n",
    "fastest_lap_details_df = fastest_lap_per_race_df.join(races_with_label, 'raceId').withColumn('fastest_lap_s', col('fastest_lap_ms') / 1000).orderBy('fastest_lap_s')\n",
    "plot_data_3 = fastest_lap_details_df.select('race_event_label', 'fastest_lap_s').toPandas()\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.barplot(x='fastest_lap_s', y='race_event_label', data=plot_data_3.head(20), orient='h', palette='plasma_r', hue='race_event_label', legend=False)\n",
    "plt.title('Volta Mais R√°pida por Corrida (Top 20)', fontsize=16)\n",
    "plt.xlabel('Tempo da Volta Mais R√°pida (segundos)', fontsize=12)\n",
    "plt.ylabel('Evento da Corrida', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fastest_race = plot_data_3.iloc[0]\n",
    "print(f\"A corrida com a volta mais r√°pida foi a '{fastest_race['race_event_label']}' com um tempo de {fastest_race['fastest_lap_s']:.3f} segundos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8da69f",
   "metadata": {},
   "source": [
    "### Tempos m√©dios por circuito\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba9784",
   "metadata": {},
   "source": [
    "- **Objetivo do gr√°fico:** Classificar os circuitos pelo tempo m√©dio de volta, do mais r√°pido ao mais lento.\n",
    "- **Dados utilizados:** M√©dias de `lap_times` por corrida combinadas com `races` e `circuits` para identificar cada pista.\n",
    "- **A√ß√µes principais:** Calcula o tempo m√©dio por corrida, associa ao `circuitId`, agrega novamente por circuito, converte para segundos, ordena e prepara um DataFrame Pandas.\n",
    "- **Sa√≠da:** Gr√°fico `Tempo M√©dio de Volta por Circuito` exibindo todas as pistas ordenadas.\n",
    "- **Insight esperado:** Mostrar quais tra√ßados favorecem velocidades elevadas ou mais baixas, apoiando compara√ß√µes de caracter√≠sticas de pista.\n",
    "- **An√°lise adicional sugerida:** Cruzar o ranking com o tipo de circuito (rua, permanente, h√≠brido) para entender padr√µes de performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524792ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col\n",
    "avg_time_per_race = dataframes['lap_times'].groupBy('raceId').agg(avg('milliseconds').alias('avg_lap_ms_race'))\n",
    "race_circuit_join = avg_time_per_race.join(dataframes['races'], 'raceId')\n",
    "avg_time_per_circuit = race_circuit_join.groupBy('circuitId').agg(avg('avg_lap_ms_race').alias('avg_circuit_ms'))\n",
    "circuit_details = avg_time_per_circuit.join(dataframes['circuits'], 'circuitId').withColumn('avg_circuit_s', col('avg_circuit_ms') / 1000).orderBy('avg_circuit_s')\n",
    "plot_data_4 = circuit_details.select('name', 'avg_circuit_s').toPandas()\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.barplot(x='avg_circuit_s', y='name', data=plot_data_4, orient='h', palette='cubehelix_r', hue='name', legend=False)\n",
    "plt.title('Tempo M√©dio de Volta por Circuito', fontsize=16)\n",
    "plt.xlabel('Tempo M√©dio da Volta (segundos)', fontsize=12)\n",
    "plt.ylabel('Circuito', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fastest_circuit = plot_data_4.iloc[0]\n",
    "slowest_circuit = plot_data_4.iloc[-1]\n",
    "print(f\"A pista mais r√°pida em m√©dia √©: '{fastest_circuit['name']}' com um tempo de {fastest_circuit['avg_circuit_s']:.3f} segundos.\")\n",
    "print(f\"A pista mais lenta em m√©dia √©: '{slowest_circuit['name']}' com um tempo de {slowest_circuit['avg_circuit_s']:.3f} segundos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7026e504",
   "metadata": {},
   "source": [
    "### Consist√™ncia vs velocidade de pilotos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c9dbaf",
   "metadata": {},
   "source": [
    "- **Objetivo do gr√°fico:** Relacionar velocidade m√©dia das voltas com consist√™ncia (baixa variabilidade) dos pilotos mais atuantes.\n",
    "- **Dados utilizados:** `lap_times` para m√©dia e desvio padr√£o por piloto/corrida e `drivers` para rotular os participantes.\n",
    "- **A√ß√µes principais:** Agrupa por corrida e piloto, calculando m√©dia e desvio padr√£o; descarta casos sem vari√¢ncia; seleciona os 15 pilotos mais presentes; converte valores para segundos; gera gr√°fico de dispers√£o e calcula o piloto de menor desvio m√©dio.\n",
    "- **Sa√≠da:** Gr√°fico `Consist√™ncia vs. Velocidade M√©dia por Piloto` e mensagem destacando o piloto mais consistente.\n",
    "- **Insight esperado:** Distinguir pilotos velozes por√©m inst√°veis daqueles que combinam rapidez com regularidade.\n",
    "- **Poss√≠veis extens√µes:** Incluir tamanho do marcador proporcional ao n√∫mero de corridas para contextualizar experi√™ncia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a55dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import stddev, avg, col, concat_ws, count\n",
    "driver_consistency_df = dataframes['lap_times'].groupBy('raceId', 'driverId').agg(stddev('milliseconds').alias('stddev_ms'), avg('milliseconds').alias('avg_ms')).filter(col('stddev_ms').isNotNull())\n",
    "drivers_with_label = dataframes['drivers'].withColumn('driver_name', concat_ws(' ', col('forename'), col('surname')))\n",
    "full_consistency_df = driver_consistency_df.join(drivers_with_label, 'driverId')\n",
    "top_drivers = full_consistency_df.groupBy('driver_name').count().orderBy(col('count').desc()).limit(15).toPandas()['driver_name'].tolist()\n",
    "plot_data_5 = full_consistency_df.withColumn('avg_s', col('avg_ms') / 1000).withColumn('stddev_s', col('stddev_ms') / 1000).select('driver_name', 'avg_s', 'stddev_s').toPandas()\n",
    "plot_data_5_filtered = plot_data_5[plot_data_5['driver_name'].isin(top_drivers)]\n",
    "plt.figure(figsize=(16, 9))\n",
    "sns.scatterplot(data=plot_data_5_filtered, x='avg_s', y='stddev_s', hue='driver_name', alpha=0.7, s=100)\n",
    "plt.title('Consist√™ncia vs. Velocidade M√©dia por Piloto', fontsize=16)\n",
    "plt.xlabel('Tempo de Volta M√©dio (segundos) -> Mais R√°pido', fontsize=12)\n",
    "plt.ylabel('Desvio Padr√£o (segundos) -> Mais Consistente', fontsize=12)\n",
    "plt.legend(title='Piloto', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "overall_consistency = full_consistency_df.groupBy('driver_name').agg(avg('stddev_ms').alias('avg_stddev')).orderBy('avg_stddev').toPandas()\n",
    "most_consistent_driver = overall_consistency.iloc[0]\n",
    "print(f\"O piloto com o ritmo mais consistente (menor desvio padr√£o m√©dio) √©: {most_consistent_driver['driver_name']} com um desvio padr√£o m√©dio de {most_consistent_driver['avg_stddev'] / 1000:.3f} segundos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c2dd0",
   "metadata": {},
   "source": [
    "### Evolu√ß√£o m√©dia dentro da corrida\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca990a",
   "metadata": {},
   "source": [
    "- **Objetivo do gr√°fico:** Entender como o tempo m√©dio de volta evolui ao longo de uma corrida t√≠pica.\n",
    "- **Dados utilizados:** `lap_times` agregados por n√∫mero de volta em toda a hist√≥ria dispon√≠vel.\n",
    "- **A√ß√µes principais:** Calcula a m√©dia por volta, converte para segundos, filtra at√© a volta 70, aplica m√©dia m√≥vel de 4 voltas e plota duas s√©ries (original e suavizada).\n",
    "- **Sa√≠da:** Gr√°fico `Evolu√ß√£o do Tempo M√©dio de Volta Durante as Corridas` com s√©rie original e m√©dia m√≥vel.\n",
    "- **Insight esperado:** Detectar fases onde os carros tendem a acelerar ou desacelerar, influenciadas por combust√≠vel, pneus ou safety cars.\n",
    "- **Sugest√£o anal√≠tica:** Comparar segmentos de voltas (ex.: 1-10, 11-20) para corroborar hip√≥teses sobre estrat√©gias de pit stop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54105e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col\n",
    "lap_evolution_df = dataframes['lap_times'].groupBy('lap').agg(avg('milliseconds').alias('avg_ms')).withColumn('avg_s', col('avg_ms') / 1000).orderBy('lap')\n",
    "plot_data_6 = lap_evolution_df.filter(col('lap') <= 70).toPandas()\n",
    "plot_data_6['rolling_avg_s'] = plot_data_6['avg_s'].rolling(window=4, min_periods=1).mean()\n",
    "plt.figure(figsize=(18, 8))\n",
    "sns.lineplot(data=plot_data_6, x='lap', y='avg_s', label='Tempo M√©dio por Volta', alpha=0.5)\n",
    "sns.lineplot(data=plot_data_6, x='lap', y='rolling_avg_s', label='M√©dia M√≥vel (4 voltas)', color='red', linewidth=2)\n",
    "plt.title('Evolu√ß√£o do Tempo M√©dio de Volta Durante as Corridas', fontsize=16)\n",
    "plt.xlabel('N√∫mero da Volta', fontsize=12)\n",
    "plt.ylabel('Tempo M√©dio de Volta (segundos)', fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bdbc17",
   "metadata": {},
   "source": [
    "### Melhor volta individual de cada piloto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0196c2",
   "metadata": {},
   "source": [
    "- **Objetivo do gr√°fico:** Listar os pilotos que registraram as voltas mais r√°pidas absolutas na base.\n",
    "- **Dados utilizados:** `lap_times` para o tempo m√≠nimo por piloto e `drivers` para montar o nome completo.\n",
    "- **A√ß√µes principais:** Calcula o menor tempo por `driverId`, junta com nomes, converte para segundos, ordena, transforma em Pandas e plota o Top 25 em barras horizontais.\n",
    "- **Sa√≠da:** Gr√°fico `Melhor Volta Pessoal de Cada Piloto (Top 25)` e impress√£o do recordista geral.\n",
    "- **Insight esperado:** Fornecer refer√™ncia de pico de performance individual, √∫til para comparar talentos ou eras.\n",
    "- **Nota contextual:** Voltas r√°pidas podem refletir compostos de pneus, regimes de combust√≠vel e regulamenta√ß√µes espec√≠ficas de cada ano.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b5a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, col, concat_ws\n",
    "fastest_lap_per_driver = dataframes['lap_times'].groupBy('driverId').agg(min('milliseconds').alias('fastest_lap_ms'))\n",
    "drivers_with_label = dataframes['drivers'].withColumn('driver_name', concat_ws(' ', col('forename'), col('surname')))\n",
    "driver_fastest_lap_details = fastest_lap_per_driver.join(drivers_with_label, 'driverId').withColumn('fastest_lap_s', col('fastest_lap_ms') / 1000).orderBy('fastest_lap_s')\n",
    "plot_data_7 = driver_fastest_lap_details.select('driver_name', 'fastest_lap_s').toPandas()\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.barplot(data=plot_data_7.head(25), x='fastest_lap_s', y='driver_name', orient='h', palette='viridis_r', hue='driver_name', legend=False)\n",
    "plt.title('Melhor Volta Pessoal de Cada Piloto (Top 25)', fontsize=16)\n",
    "plt.xlabel('Tempo da Volta Mais R√°pida (segundos)', fontsize=12)\n",
    "plt.ylabel('Piloto', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fastest_driver_overall = plot_data_7.iloc[0]\n",
    "print(f\"O piloto com a volta mais r√°pida de todos os tempos na base de dados √© {fastest_driver_overall['driver_name']} com o tempo de {fastest_driver_overall['fastest_lap_s']:.3f} segundos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9785885b",
   "metadata": {},
   "source": [
    "### Desempenho m√©dio por construtor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cee1f63",
   "metadata": {},
   "source": [
    "- **Objetivo do gr√°fico:** Avaliar quais construtores registram menor tempo m√©dio de volta.\n",
    "- **Dados utilizados:** `lap_times` combinados com `results` (para mapear construtor) e `constructors` (nome da equipe).\n",
    "- **A√ß√µes principais:** Cria o mapa corrida/piloto/construtor, associa aos tempos de volta, calcula a m√©dia por equipe, converte para segundos, ordena e plota o Top 25.\n",
    "- **Sa√≠da:** Gr√°fico `Tempo M√©dio de Volta por Construtor (Top 25)` e mensagem apontando a equipe mais veloz em m√©dia.\n",
    "- **Insight esperado:** Compreender a domin√¢ncia t√©cnica das equipes em ritmo de corrida.\n",
    "- **Ponto de aten√ß√£o:** Diferen√ßas na quantidade de temporadas disputadas podem distorcer a m√©dia; considerar normaliza√ß√£o futura.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e83758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col\n",
    "race_driver_constructor_map = dataframes['results'].select('raceId', 'driverId', 'constructorId').distinct()\n",
    "laps_with_constructors = dataframes['lap_times'].join(race_driver_constructor_map, on=['raceId', 'driverId'], how='inner')\n",
    "avg_time_per_constructor = laps_with_constructors.groupBy('constructorId').agg(avg('milliseconds').alias('avg_ms'))\n",
    "constructor_lap_details = avg_time_per_constructor.join(dataframes['constructors'], 'constructorId').withColumn('avg_s', col('avg_ms') / 1000).orderBy('avg_s')\n",
    "plot_data_8 = constructor_lap_details.select('name', 'avg_s').toPandas()\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.barplot(data=plot_data_8.head(25), x='avg_s', y='name', orient='h', palette='magma_r', hue='name', legend=False)\n",
    "plt.title('Tempo M√©dio de Volta por Construtor (Top 25)', fontsize=16)\n",
    "plt.xlabel('Tempo M√©dio da Volta (segundos)', fontsize=12)\n",
    "plt.ylabel('Construtor', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fastest_constructor = plot_data_8.iloc[0]\n",
    "print(f\"A equipe com os carros mais r√°pidos em m√©dia √©: {fastest_constructor['name']} com um tempo m√©dio de volta de {fastest_constructor['avg_s']:.3f} segundos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f563ccbe",
   "metadata": {},
   "source": [
    "### Velocidade m√©dia por pa√≠s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b07a7e0",
   "metadata": {},
   "source": [
    "- **Objetivo do gr√°fico:** Rankear pa√≠ses pelos tempos m√©dios das voltas realizadas em seus circuitos.\n",
    "- **Dados utilizados:** `lap_times` (m√©dias por corrida) aliados a `races` e `circuits` para identificar o pa√≠s.\n",
    "- **A√ß√µes principais:** Liga cada corrida ao pa√≠s, agrega os tempos m√©dios por pa√≠s, converte para segundos, ordena e cria DataFrame Pandas para visualiza√ß√£o.\n",
    "- **Sa√≠da:** Gr√°fico `Velocidade M√©dia por Pa√≠s do Circuito` abrangendo todos os pa√≠ses presentes.\n",
    "- **Insight esperado:** Indicar localiza√ß√µes onde as pistas tendem a ser mais r√°pidas ou lentas, auxiliando compara√ß√µes geogr√°ficas.\n",
    "- **An√°lise complementar:** Cruzar o ranking com dados de clima, altitude ou layout para explicar diferen√ßas observadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac5e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col\n",
    "avg_time_per_race = dataframes['lap_times'].groupBy('raceId').agg(avg('milliseconds').alias('avg_lap_ms_race'))\n",
    "race_circuit_join = avg_time_per_race.join(dataframes['races'], 'raceId')\n",
    "race_country_join = race_circuit_join.join(dataframes['circuits'], 'circuitId')\n",
    "avg_time_per_country = race_country_join.groupBy('country').agg(avg('avg_lap_ms_race').alias('avg_country_ms')).withColumn('avg_country_s', col('avg_country_ms') / 1000).orderBy('avg_country_s')\n",
    "plot_data_9 = avg_time_per_country.select('country', 'avg_country_s').toPandas()\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.barplot(data=plot_data_9, x='avg_country_s', y='country', orient='h', palette='cividis_r', hue='country', legend=False)\n",
    "plt.title('Velocidade M√©dia por Pa√≠s do Circuito', fontsize=16)\n",
    "plt.xlabel('Tempo M√©dio da Volta (segundos)', fontsize=12)\n",
    "plt.ylabel('Pa√≠s', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fastest_country = plot_data_9.iloc[0]\n",
    "print(f\"As pistas mais velozes, em m√©dia, est√£o na: {fastest_country['country']} com um tempo m√©dio de volta de {fastest_country['avg_country_s']:.3f} segundos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9b1b5",
   "metadata": {},
   "source": [
    "### Ganho ou perda entre voltas consecutivas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b535ef31",
   "metadata": {},
   "source": [
    "- **Objetivo do gr√°fico:** Quantificar o ganho ou perda m√©dia ao comparar cada volta com a imediatamente anterior.\n",
    "- **Dados utilizados:** `lap_times` tratados com janelas (`Window`) particionadas por corrida e piloto.\n",
    "- **A√ß√µes principais:** Usa `lag` para recuperar a volta anterior, calcula a diferen√ßa, remove valores nulos, agrega por n√∫mero de volta, limita √†s 70 primeiras e converte diferen√ßas para segundos.\n",
    "- **Sa√≠da:** Gr√°fico `Diferen√ßa M√©dia Entre Voltas Consecutivas` com colora√ß√£o distinta para ganhos e perdas.\n",
    "- **Insight esperado:** Destacar momentos da corrida onde o ritmo coletivo tende a cair ou melhorar (pit stops, safety car, desgaste de pneus).\n",
    "- **Leitura recomendada:** Observar mudan√ßas de sinal para detectar voltas cr√≠ticas em estrat√©gia de corrida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f276c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, col, avg\n",
    "window_spec = Window.partitionBy('raceId', 'driverId').orderBy('lap')\n",
    "laps_with_previous = dataframes['lap_times'].withColumn('prev_lap_ms', lag('milliseconds', 1).over(window_spec))\n",
    "lap_delta_df = laps_with_previous.withColumn('delta_ms', col('milliseconds') - col('prev_lap_ms')).filter(col('delta_ms').isNotNull())\n",
    "avg_delta_per_lap = lap_delta_df.groupBy('lap').agg(avg('delta_ms').alias('avg_delta_ms')).withColumn('avg_delta_s', col('avg_delta_ms') / 1000).orderBy('lap')\n",
    "plot_data_10 = avg_delta_per_lap.filter(col('lap') <= 70).toPandas()\n",
    "plt.figure(figsize=(18, 8))\n",
    "palette = ['crimson' if x > 0 else 'limegreen' for x in plot_data_10['avg_delta_s']]\n",
    "sns.barplot(data=plot_data_10, x='lap', y='avg_delta_s', palette=palette)\n",
    "plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "plt.title('Diferen√ßa M√©dia Entre Voltas Consecutivas', fontsize=16)\n",
    "plt.xlabel('N√∫mero da Volta (Transi√ß√£o de N-1 para N)', fontsize=12)\n",
    "plt.ylabel('Diferen√ßa de Tempo (segundos)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c24971c",
   "metadata": {},
   "source": [
    "### Posi√ß√£o m√©dia de pilotos experientes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ed01af",
   "metadata": {},
   "source": [
    "- **Objetivo do gr√°fico:** Comparar a posi√ß√£o m√©dia de chegada entre pilotos com grande volume de voltas acumuladas.\n",
    "- **Dados utilizados:** `lap_times` para posi√ß√µes e contagem de voltas, `drivers` para rotular.\n",
    "- **A√ß√µes principais:** Filtra pilotos com mais de 2000 voltas, calcula a posi√ß√£o m√©dia, associa nomes, ordena, converte para Pandas e plota o Top 25.\n",
    "- **Sa√≠da:** Gr√°fico `Posi√ß√£o M√©dia em Corrida por Piloto (com mais de 2000 voltas)` e mensagem com o piloto de melhor m√©dia.\n",
    "- **Insight esperado:** Destacar competidores que se mant√™m regularmente nas primeiras posi√ß√µes quando t√™m amostra representativa.\n",
    "- **Considera√ß√£o adicional:** Cruzar com equipes ou per√≠odos hist√≥ricos para contextualizar a m√©dia encontrada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a7643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col, count, concat_ws\n",
    "experienced_drivers = dataframes['lap_times'].groupBy('driverId').agg(count('*').alias('total_laps')).filter(col('total_laps') > 2000).select('driverId')\n",
    "avg_position_per_driver = dataframes['lap_times'].join(experienced_drivers, 'driverId', 'inner').groupBy('driverId').agg(avg('position').alias('avg_position'))\n",
    "drivers_with_label = dataframes['drivers'].withColumn('driver_name', concat_ws(' ', col('forename'), col('surname')))\n",
    "driver_avg_position_details = avg_position_per_driver.join(drivers_with_label, 'driverId').orderBy('avg_position')\n",
    "plot_data_11 = driver_avg_position_details.select('driver_name', 'avg_position').toPandas()\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.barplot(data=plot_data_11.head(25), x='avg_position', y='driver_name', orient='h', palette='rocket_r', hue='driver_name', legend=False)\n",
    "plt.xlim(left=1)\n",
    "plt.title('Posi√ß√£o M√©dia em Corrida por Piloto (com mais de 2000 voltas)', fontsize=16)\n",
    "plt.xlabel('Posi√ß√£o M√©dia', fontsize=12)\n",
    "plt.ylabel('Piloto', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "most_stable_driver = plot_data_11.iloc[0]\n",
    "print(f\"O piloto com a melhor posi√ß√£o m√©dia em corrida, indicando maior estabilidade na lideran√ßa, √©: {most_stable_driver['driver_name']} com uma posi√ß√£o m√©dia de {most_stable_driver['avg_position']:.2f}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c0b3ca",
   "metadata": {},
   "source": [
    "### Total de voltas por piloto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a9767a",
   "metadata": {},
   "source": [
    "- **Objetivo do gr√°fico:** Medir a quilometragem total de cada piloto em n√∫mero de voltas completadas.\n",
    "- **Dados utilizados:** `lap_times` para contagem de voltas e `drivers` para nomes.\n",
    "- **A√ß√µes principais:** Agrupa por piloto, conta o total de voltas, associa nomes, ordena, converte para Pandas e plota o Top 25.\n",
    "- **Sa√≠da:** Gr√°fico `Total de Voltas Completadas por Piloto (Top 25)` e texto indicando o piloto mais longevo.\n",
    "- **Insight esperado:** Evidenciar resist√™ncia e perman√™ncia na categoria, base para analisar carreiras longas.\n",
    "- **Complemento sugerido:** Relacionar com n√∫mero de corridas disputadas ou temporadas para obter m√©tricas proporcionais.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e32ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col, concat_ws\n",
    "total_laps_per_driver = dataframes['lap_times'].groupBy('driverId').agg(count('*').alias('total_laps'))\n",
    "drivers_with_label = dataframes['drivers'].withColumn('driver_name', concat_ws(' ', col('forename'), col('surname')))\n",
    "driver_laps_details = total_laps_per_driver.join(drivers_with_label, 'driverId').orderBy(col('total_laps').desc())\n",
    "plot_data_12 = driver_laps_details.select('driver_name', 'total_laps').toPandas()\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.barplot(data=plot_data_12.head(25), x='total_laps', y='driver_name', orient='h', palette='plasma', hue='driver_name', legend=False)\n",
    "plt.title('Total de Voltas Completadas por Piloto (Top 25)', fontsize=16)\n",
    "plt.xlabel('N√∫mero Total de Voltas', fontsize=12)\n",
    "plt.ylabel('Piloto', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "most_laps_driver = plot_data_12.iloc[0]\n",
    "print(f\"O piloto que completou mais voltas, demonstrando grande longevidade e resist√™ncia, √©: {most_laps_driver['driver_name']} com um total de {most_laps_driver['total_laps']} voltas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56366ea3",
   "metadata": {},
   "source": [
    "### Total de voltas por piloto (varia√ß√£o com salvamento)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ff286",
   "metadata": {},
   "source": [
    "- **Objetivo do gr√°fico:** Disponibilizar a mesma m√©trica de voltas completadas em um bloco pronto para ajustes de salvamento da figura.\n",
    "- **Dados utilizados:** `lap_times` e `drivers`, reaproveitando a mesma estrutura da c√©lula anterior.\n",
    "- **A√ß√µes principais:** Reconta voltas por piloto com importa√ß√µes expl√≠citas, transforma em Pandas e plota o Top 25 em barras horizontais.\n",
    "- **Sa√≠da:** Gr√°fico `Total de Voltas Completadas por Piloto (Top 25)` replicado, facilitando exporta√ß√µes personalizadas.\n",
    "- **Insight esperado:** Garantir uma vers√£o facilmente adapt√°vel da visualiza√ß√£o para apresenta√ß√µes ou relat√≥rios.\n",
    "- **Observa√ß√£o:** Pode ser ajustado para outros filtros (pilotos ativos, equipes) sem afetar an√°lises anteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13378a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import count, col, concat_ws\n",
    "total_laps_per_driver = dataframes['lap_times'].groupBy('driverId').agg(count('*').alias('total_laps'))\n",
    "drivers_with_label = dataframes['drivers'].withColumn('driver_name', concat_ws(' ', col('forename'), col('surname')))\n",
    "driver_laps_details = total_laps_per_driver.join(drivers_with_label, 'driverId').orderBy(col('total_laps').desc())\n",
    "plot_data_12 = driver_laps_details.select('driver_name', 'total_laps').toPandas()\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.barplot(data=plot_data_12.head(25), x='total_laps', y='driver_name', orient='h', palette='plasma', hue='driver_name', legend=False)\n",
    "plt.title('Total de Voltas Completadas por Piloto (Top 25)', fontsize=16)\n",
    "plt.xlabel('N√∫mero Total de Voltas', fontsize=12)\n",
    "plt.ylabel('Piloto', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "most_laps_driver = plot_data_12.iloc[0]\n",
    "print(f\"O piloto que completou mais voltas, demonstrando grande longevidade e resist√™ncia, √©: {most_laps_driver['driver_name']} com um total de {most_laps_driver['total_laps']} voltas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2306b8bb",
   "metadata": {},
   "source": [
    "### Dispers√£o de performance por circuito\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb65a52",
   "metadata": {},
   "source": [
    "- **Objetivo do gr√°fico:** Avaliar o qu√£o disputado ou dominado √© cada circuito medindo a dispers√£o dos tempos m√©dios dos pilotos.\n",
    "- **Dados utilizados:** `lap_times` combinados com `races` e `circuits` para ligar pilotos a circuitos.\n",
    "- **A√ß√µes principais:** Calcula o tempo m√©dio de cada piloto por circuito, agrega o desvio padr√£o por pista, converte para segundos, ordena e plota o ranking completo.\n",
    "- **Sa√≠da:** Gr√°fico `Dispers√£o de Desempenho (Desvio Padr√£o) Entre Pilotos por Circuito` e mensagens apontando circuitos extremos.\n",
    "- **Insight esperado:** Diferenciar pistas dominadas por poucos pilotos (desvio alto) de pistas equilibradas (desvio baixo).\n",
    "- **Follow-up sugerido:** Integrar estat√≠sticas de ultrapassagens ou clima para justificar a dispers√£o observada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b192942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, stddev, col\n",
    "laps_with_races = dataframes['lap_times'].join(dataframes['races'], 'raceId')\n",
    "avg_time_per_driver_circuit = laps_with_races.groupBy('circuitId', 'driverId').agg(avg('milliseconds').alias('avg_driver_circuit_ms'))\n",
    "performance_spread_per_circuit = avg_time_per_driver_circuit.groupBy('circuitId').agg(stddev('avg_driver_circuit_ms').alias('spread_ms'))\n",
    "spread_details = performance_spread_per_circuit.join(dataframes['circuits'], 'circuitId').withColumn('spread_s', col('spread_ms') / 1000).orderBy(col('spread_s').desc())\n",
    "plot_data_14 = spread_details.select('name', 'spread_s').toPandas()\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.barplot(data=plot_data_14, x='spread_s', y='name', orient='h', palette='coolwarm', hue='name', legend=False)\n",
    "plt.title('Dispers√£o de Desempenho (Desvio Padr√£o) Entre Pilotos por Circuito', fontsize=16)\n",
    "plt.xlabel('Desvio Padr√£o dos Tempos M√©dios de Volta (segundos)', fontsize=12)\n",
    "plt.ylabel('Circuito', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "most_disputed_circuit = plot_data_14.iloc[-1]\n",
    "most_dominated_circuit = plot_data_14.iloc[0]\n",
    "print(f\"O circuito com a maior dispers√£o de tempos (sugest√£o de dom√≠nio) √©: '{most_dominated_circuit['name']}' com um desvio padr√£o de {most_dominated_circuit['spread_s']:.3f} segundos.\")\n",
    "print(f\"O circuito com a menor dispers√£o de tempos (sugest√£o de disputa acirrada) √©: '{most_disputed_circuit['name']}' com um desvio padr√£o de {most_disputed_circuit['spread_s']:.3f} segundos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e789f02",
   "metadata": {},
   "source": [
    "### Evolu√ß√£o hist√≥rica do tempo m√©dio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5229dac",
   "metadata": {},
   "source": [
    "- **Objetivo do gr√°fico:** Acompanhar a evolu√ß√£o do tempo m√©dio de volta ao longo das temporadas.\n",
    "- **Dados utilizados:** `lap_times` enriquecidos com o ano proveniente de `races`.\n",
    "- **A√ß√µes principais:** Junta voltas ao ano da corrida, calcula a m√©dia por temporada, converte para segundos, ordena cronologicamente e plota uma s√©rie temporal com marcadores.\n",
    "- **Sa√≠da:** Gr√°fico `Evolu√ß√£o do Tempo M√©dio de Volta por Temporada` com eixo X formatado em anos inteiros.\n",
    "- **Insight esperado:** Visualizar tend√™ncias de ganho ou perda de performance ligadas a mudan√ßas de regulamento, tecnologia e pneus.\n",
    "- **Pr√≥ximo passo sugerido:** Anotar anos de grandes mudan√ßas t√©cnicas para contextualizar saltos ou quedas abruptas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb38d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, col\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "laps_with_year = dataframes['lap_times'].join(dataframes['races'], 'raceId')\n",
    "avg_time_per_season = laps_with_year.groupBy('year').agg(avg('milliseconds').alias('avg_ms')).withColumn('avg_s', col('avg_ms') / 1000).orderBy('year')\n",
    "plot_data_15 = avg_time_per_season.toPandas()\n",
    "plt.figure(figsize=(18, 9))\n",
    "ax = sns.lineplot(data=plot_data_15, x='year', y='avg_s', marker='o', linewidth=2.5)\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Evolu√ß√£o do Tempo M√©dio de Volta por Temporada', fontsize=16)\n",
    "plt.xlabel('Temporada (Ano)', fontsize=12)\n",
    "plt.ylabel('Tempo M√©dio de Volta (segundos)', fontsize=12)\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f8996b",
   "metadata": {},
   "source": [
    "### Detec√ß√£o de voltas outlier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88767cd3",
   "metadata": {},
   "source": [
    "- **Objetivo do gr√°fico:** Identificar voltas anormalmente lentas e quantificar quais pilotos mais sofreram com essas ocorr√™ncias.\n",
    "- **Dados utilizados:** `lap_times` para m√©dias e desvios por corrida, al√©m de `drivers` e `races` para enriquecer os detalhes.\n",
    "- **A√ß√µes principais:** Calcula m√©dia e desvio padr√£o por corrida, define limite de outlier (m√©dia + 2 desvios), filtra voltas acima do limiar, adiciona nomes e evento, agrupa por piloto e monta a tabela das 20 piores voltas.\n",
    "- **Sa√≠da:** Gr√°fico `Contagem de Voltas \"Outlier\" (Muito Lentas) por Piloto` e tabela formatada `Top 20 Voltas Mais Lentas`.\n",
    "- **Insight esperado:** Evidenciar pilotos e corridas onde ocorreram problemas significativos (falhas mec√¢nicas, incidentes, penalidades).\n",
    "- **Uso pr√°tico:** Apoiar relat√≥rios de corrida destacando momentos cr√≠ticos e avaliar confiabilidade de pilotos/equipes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487086b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, stddev, col, count, concat_ws\n",
    "race_stats = dataframes['lap_times'].groupBy('raceId').agg(avg('milliseconds').alias('avg_ms_race'), stddev('milliseconds').alias('stddev_ms_race'))\n",
    "laps_with_stats = dataframes['lap_times'].join(race_stats, 'raceId')\n",
    "outlier_threshold = col('avg_ms_race') + 2 * col('stddev_ms_race')\n",
    "outlier_laps = laps_with_stats.filter(col('milliseconds') > outlier_threshold)\n",
    "drivers_with_label = dataframes['drivers'].withColumn('driver_name', concat_ws(' ', col('forename'), col('surname')))\n",
    "races_with_label = dataframes['races'].withColumn('race_event', concat_ws(' ', col('name'), col('year')))\n",
    "outliers_with_details = outlier_laps.join(drivers_with_label, 'driverId').join(races_with_label, 'raceId').withColumn('lap_time_s', col('milliseconds') / 1000).withColumn('race_avg_s', col('avg_ms_race') / 1000).withColumn('slower_by_s', (col('milliseconds') - col('avg_ms_race')) / 1000).orderBy(col('slower_by_s').desc())\n",
    "outlier_counts_per_driver = outliers_with_details.groupBy('driver_name').agg(count('*').alias('outlier_count')).orderBy(col('outlier_count').desc())\n",
    "plot_data_16 = outlier_counts_per_driver.toPandas()\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.barplot(data=plot_data_16.head(25), x='outlier_count', y='driver_name', orient='h', palette='autumn_r', hue='driver_name', legend=False)\n",
    "plt.title('Contagem de Voltas \"Outlier\" (Muito Lentas) por Piloto', fontsize=16)\n",
    "plt.xlabel('N√∫mero de Voltas Outlier', fontsize=12)\n",
    "plt.ylabel('Piloto', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "top_20_outliers_pd = outliers_with_details.select('race_event', 'driver_name', 'lap', 'lap_time_s', 'race_avg_s', 'slower_by_s').limit(20).toPandas()\n",
    "print('\\n--- Top 20 Voltas Mais Lentas (em rela√ß√£o √† m√©dia da corrida) ---')\n",
    "top_20_outliers_pd['lap_time_s'] = top_20_outliers_pd['lap_time_s'].map('{:,.2f}s'.format)\n",
    "top_20_outliers_pd['race_avg_s'] = top_20_outliers_pd['race_avg_s'].map('{:,.2f}s'.format)\n",
    "top_20_outliers_pd['slower_by_s'] = top_20_outliers_pd['slower_by_s'].map('+{:,.2f}s'.format)\n",
    "display(top_20_outliers_pd)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
