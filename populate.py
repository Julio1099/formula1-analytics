from pyspark.sql import SparkSession
from pyspark.sql.utils import AnalysisException

# ğŸ”¹ ConfiguraÃ§Ã£o da SparkSession
spark = SparkSession.builder \
    .appName("PopulateFormula1") \
    .config("spark.jars.packages", "org.postgresql:postgresql:42.6.0") \
    .getOrCreate()

# ğŸ”¹ ConfiguraÃ§Ã£o do PostgreSQL
jdbc_url = "jdbc:postgresql://db:5432/formula1"
db_properties = {
    "user": "postgres",
    "password": "postgres",
    "driver": "org.postgresql.Driver"
}

# ğŸ”¹ Lista das tabelas (pastas Parquet)
tabelas = [
    "constructors",
    "drivers",
    "races",
    "lap_times_fact",
    "pit_stops",   
    "results",     
    "status"
]

# ğŸ”¹ FunÃ§Ã£o auxiliar para leitura e escrita
def carregar_e_inserir(nome_tabela):
    caminho = f"/dados/{nome_tabela}"
    print(f"ğŸ“‚ Lendo dados de: {caminho}")
    try:
        df = spark.read.parquet(caminho)
        print(f"âœ… {nome_tabela}: {df.count()} linhas carregadas")

        # Escrever no PostgreSQL
        df.write.jdbc(
            url=jdbc_url,
            table=nome_tabela,
            mode="overwrite",
            properties=db_properties
        )
        print(f"ğŸ’¾ Dados inseridos em: {nome_tabela}")

    except AnalysisException as e:
        print(f"âš ï¸ Erro ao ler {nome_tabela}: {e}")
    except Exception as e:
        print(f"âŒ Erro ao inserir {nome_tabela}: {e}")

# ğŸ”¹ Executar o pipeline
for tabela in tabelas:
    carregar_e_inserir(tabela)

print("ğŸ Processo de carga concluÃ­do com sucesso!")

spark.stop()
