{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a2b1e6",
   "metadata": {},
   "source": [
    "# Job ETL: Raw -> Silver\n",
    "\n",
    "\n",
    "Notebook responsável por **extrair os dados da camada Raw**, **realizar limpezas, padronizações e remoção de outliers**, e **carregar os resultados tratados na camada Silver**.  \n",
    "Execute as células na ordem apresentada e ajuste apenas os parâmetros indicados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca12d12",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Congurações Iniciais\n",
    "\n",
    "### 1.1 Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c836b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, when, split\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7658861",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 Configuração do Ambiente Hadoop para Windows\n",
    "\n",
    "Para permitir o correto funcionamento do Spark no Windows, é necessária a configuração do ambiente Hadoop local, apontando para os binários de suporte (`winutils.exe` e `hadoop.dll`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae1961bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_HOME'] = 'C:\\\\hadoop'\n",
    "os.environ['PATH'] = f\"{os.environ['HADOOP_HOME']}\\\\bin;{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96511977",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3 Limpeza de Diretório Temporário\n",
    "\n",
    "Antes da execução, o diretório temporário do Spark é limpo e recriado, garantindo que não haja resíduos de execuções anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c275c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = \"C:\\\\spark-temp\"\n",
    "if os.path.exists(temp_dir):\n",
    "    shutil.rmtree(temp_dir)\n",
    "os.makedirs(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b1e331",
   "metadata": {},
   "source": [
    "\n",
    "### 1.4 Inicialização da SparkSession \n",
    "\n",
    "A `SparkSession` é criada com parâmetros específicos para definir os diretórios locais e de warehouse dentro do diretório temporário criado anteriormente.  \n",
    "Essas configurações evitam erros de caminho e melhoram a compatibilidade em ambientes Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae1013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir_uri = temp_dir.replace('\\\\', '/')\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Formula1_Raw_to_Silver_Outliers\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", f\"{temp_dir_uri}/warehouse\") \\\n",
    "    .config(\"spark.local.dir\", f\"{temp_dir_uri}/local\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b2759",
   "metadata": {},
   "source": [
    "\n",
    "### 1.5 Iniciação dos Caminhos das Camadas\n",
    "\n",
    "Os diretórios de origem (Raw) e destino (Silver) são definidos.  \n",
    "O caminho de destino é criado automaticamente, caso não exista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "024eb51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório de origem (RAW): C:\\Users\\julii\\OneDrive\\Documentos\\formula1-analytics\\Data Layer\\raw\\dados_originais\n",
      "Diretório de destino (SILVER): C:\\Users\\julii\\OneDrive\\Documentos\\formula1-analytics\\Data Layer\\silver\\dados_limpos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_path = os.path.join(\"C:\", os.sep, \"Users\", \"julii\", \"OneDrive\", \"Documentos\", \"formula1-analytics\", \"Data Layer\")\n",
    "raw_path = os.path.join(base_path, \"raw\", \"dados_originais\")\n",
    "silver_path = os.path.join(base_path, \"silver\", \"dados_limpos\")\n",
    "\n",
    "os.makedirs(silver_path, exist_ok=True)\n",
    "print(f\"Diretório de origem (RAW): {raw_path}\")\n",
    "print(f\"Diretório de destino (SILVER): {silver_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f577826",
   "metadata": {},
   "source": [
    "\n",
    "### 1.6 Leitura dos Dados da Camada RAW\n",
    "\n",
    "Todos os arquivos CSV da camada Raw são carregados em DataFrames Spark.  \n",
    "O parâmetro `nullValue=\"\\\\N\"` garante que valores nulos sejam interpretados corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "689a9227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lendo arquivos da camada RAW com tratamento de nulos (\\N)... \n",
      "\n",
      " Arquivos RAW carregados com sucesso!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\" Lendo arquivos da camada RAW com tratamento de nulos (\\\\N)... \\n\")\n",
    "\n",
    "df_lap_times = spark.read.option(\"nullValue\", \"\\\\N\").csv(os.path.join(raw_path, \"lap_times.csv\"), header=True, inferSchema=True)\n",
    "df_results = spark.read.option(\"nullValue\", \"\\\\N\").csv(os.path.join(raw_path, \"results.csv\"), header=True, inferSchema=True)\n",
    "df_races = spark.read.option(\"nullValue\", \"\\\\N\").csv(os.path.join(raw_path, \"races.csv\"), header=True, inferSchema=True)\n",
    "df_drivers = spark.read.option(\"nullValue\", \"\\\\N\").csv(os.path.join(raw_path, \"drivers.csv\"), header=True, inferSchema=True)\n",
    "df_constructors = spark.read.option(\"nullValue\", \"\\\\N\").csv(os.path.join(raw_path, \"constructors.csv\"), header=True, inferSchema=True)\n",
    "df_status = spark.read.option(\"nullValue\", \"\\\\N\").csv(os.path.join(raw_path, \"status.csv\"), header=True, inferSchema=True)\n",
    "df_pit_stops = spark.read.option(\"nullValue\", \"\\\\N\").csv(os.path.join(raw_path, \"pit_stops.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "print(\" Arquivos RAW carregados com sucesso!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2442e3f4",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Remoção de Outliers\n",
    "\n",
    "Nesta etapa, com base no método **IQR (Interquartile Range)**, ocorre a padronização dos dados e a eliminação dos outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d190811",
   "metadata": {},
   "source": [
    "### 2.1 Tabela Corridas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d52c946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "races_silver = df_races.select(\n",
    "    col(\"raceId\").alias(\"id_corrida\"),\n",
    "    col(\"year\").alias(\"ano\"),\n",
    "    col(\"round\").alias(\"rodada\"),\n",
    "    col(\"name\").alias(\"nome_corrida\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797a52d",
   "metadata": {},
   "source": [
    "### 2.2 Tabela Pilotos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "811717d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers_silver = df_drivers.select(\n",
    "    col(\"driverId\").alias(\"id_piloto\"),\n",
    "    col(\"forename\").alias(\"primeiro_nome_piloto\"),\n",
    "    col(\"surname\").alias(\"sobrenome_piloto\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3d67dd",
   "metadata": {},
   "source": [
    "### 2.2 Tabela Equipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aba89c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "constructors_silver = df_constructors.select(\n",
    "    col(\"constructorId\").alias(\"id_equipe\"),\n",
    "    col(\"name\").alias(\"nome_equipe\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22cfbe",
   "metadata": {},
   "source": [
    "### 2.3 Tabela Pit Stops\n",
    "A coluna `duration` é convertida para segundos, tratando valores com ou sem formato “min:seg”.  \n",
    "Após isso, são removidos outliers que ultrapassam os limites definidos pelo IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92ad5c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    → Removendo outliers de 'pit_stops' (duração)...\n"
     ]
    }
   ],
   "source": [
    "pit_stops_cleaned = df_pit_stops.withColumn(\n",
    "    \"duracao_parada_seg\",\n",
    "    when(\n",
    "        col(\"duration\").contains(\":\"), \n",
    "        split(col(\"duration\"), \":\").getItem(0).cast(\"double\") * 60 +\n",
    "        split(col(\"duration\"), \":\").getItem(1).cast(\"double\")\n",
    "    ).otherwise(\n",
    "        col(\"duration\").cast(\"double\")\n",
    "    )\n",
    ")\n",
    "\n",
    "pit_stops_silver = pit_stops_cleaned.select(\n",
    "    col(\"raceId\").alias(\"id_corrida\"),\n",
    "    col(\"driverId\").alias(\"id_piloto\"),\n",
    "    \"duracao_parada_seg\"\n",
    ").na.drop(subset=[\"duracao_parada_seg\"])\n",
    "\n",
    "print(\"    → Removendo outliers de 'pit_stops' (duração)...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05644646",
   "metadata": {},
   "source": [
    "### 2.4 Tabela Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b61a2693",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_silver = df_status.select(\n",
    "    col(\"statusId\").alias(\"id_status\"),\n",
    "    col(\"status\").alias(\"descricao_status\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e93471",
   "metadata": {},
   "source": [
    "### 2.5 Tabela Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bfc1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_silver = df_results.select(\n",
    "    col(\"raceId\").alias(\"id_corrida\"),\n",
    "    col(\"driverId\").alias(\"id_piloto\"),\n",
    "    col(\"constructorId\").alias(\"id_equipe\"),\n",
    "    col(\"statusId\").alias(\"id_status\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137655f8",
   "metadata": {},
   "source": [
    "### 2.6 Tabela Tempos de Volta (Fato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "163edaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lap_times_fact = df_lap_times.select(\n",
    "    col(\"raceId\").alias(\"id_corrida\"),\n",
    "    col(\"driverId\").alias(\"id_piloto\"),\n",
    "    col(\"lap\").alias(\"volta\"),\n",
    "    col(\"position\").alias(\"posicao_na_volta\"),\n",
    "    expr(\"try_cast(milliseconds as integer)\").alias(\"tempo_volta_ms\")\n",
    ").na.drop(subset=[\"tempo_volta_ms\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1626802f",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Carga em Formato Parquet\n",
    "\n",
    "Os DataFrames processados são salvos no formato **Parquet**, o que melhora o desempenho e a compressão, facilitando consultas posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6a42857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Salvando todas as tabelas na camada SILVER em formato PARQUET...\n",
      "\n",
      "  → Dados salvos em Parquet: C:\\Users\\julii\\OneDrive\\Documentos\\formula1-analytics\\Data Layer\\silver\\dados_limpos\\races\n",
      "  → Dados salvos em Parquet: C:\\Users\\julii\\OneDrive\\Documentos\\formula1-analytics\\Data Layer\\silver\\dados_limpos\\drivers\n",
      "  → Dados salvos em Parquet: C:\\Users\\julii\\OneDrive\\Documentos\\formula1-analytics\\Data Layer\\silver\\dados_limpos\\constructors\n",
      "  → Dados salvos em Parquet: C:\\Users\\julii\\OneDrive\\Documentos\\formula1-analytics\\Data Layer\\silver\\dados_limpos\\pit_stops\n",
      "  → Dados salvos em Parquet: C:\\Users\\julii\\OneDrive\\Documentos\\formula1-analytics\\Data Layer\\silver\\dados_limpos\\status\n",
      "  → Dados salvos em Parquet: C:\\Users\\julii\\OneDrive\\Documentos\\formula1-analytics\\Data Layer\\silver\\dados_limpos\\results\n",
      "  → Dados salvos em Parquet: C:\\Users\\julii\\OneDrive\\Documentos\\formula1-analytics\\Data Layer\\silver\\dados_limpos\\lap_times_fact\n",
      "\n",
      " Camada Silver gerada com sucesso!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- COLOQUE TUDO NA MESMA CÉLULA ---\n",
    "\n",
    "print(\" Salvando todas as tabelas na camada SILVER em formato PARQUET...\\n\")\n",
    "\n",
    "# 1. Definição do dicionário\n",
    "silver_tables = {\n",
    "    \"races\": races_silver,\n",
    "    \"drivers\": drivers_silver,\n",
    "    \"constructors\": constructors_silver,\n",
    "    \"pit_stops\": pit_stops_silver,\n",
    "    \"status\": status_silver,\n",
    "    \"results\": results_silver,\n",
    "    \"lap_times_fact\": lap_times_fact\n",
    "}\n",
    "\n",
    "# 2. Loop para salvar os dados\n",
    "for name, df in silver_tables.items():\n",
    "    output_path = os.path.join(silver_path, name)\n",
    "    df.coalesce(1).write.mode(\"overwrite\").parquet(output_path)\n",
    "    print(f\"  → Dados salvos em Parquet: {output_path}\")\n",
    "\n",
    "print(\"\\n Camada Silver gerada com sucesso!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bb4c70",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Validação\n",
    "\n",
    "Por fim, é exibido o schema e uma amostra da tabela fato principal, confirmando o sucesso do processo de transformação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dbfddc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validando a tabela fato principal 'lap_times_fact':\n",
      "\n",
      "root\n",
      " |-- id_corrida: integer (nullable = true)\n",
      " |-- id_piloto: integer (nullable = true)\n",
      " |-- volta: integer (nullable = true)\n",
      " |-- posicao_na_volta: integer (nullable = true)\n",
      " |-- tempo_volta_ms: integer (nullable = true)\n",
      "\n",
      "+----------+---------+-----+----------------+--------------+\n",
      "|id_corrida|id_piloto|volta|posicao_na_volta|tempo_volta_ms|\n",
      "+----------+---------+-----+----------------+--------------+\n",
      "|       841|       20|    1|               1|         98109|\n",
      "|       841|       20|    2|               1|         93006|\n",
      "|       841|       20|    3|               1|         92713|\n",
      "|       841|       20|    4|               1|         92803|\n",
      "|       841|       20|    5|               1|         92342|\n",
      "|       841|       20|    6|               1|         92605|\n",
      "|       841|       20|    7|               1|         92502|\n",
      "|       841|       20|    8|               1|         92537|\n",
      "|       841|       20|    9|               1|         93240|\n",
      "|       841|       20|   10|               1|         92572|\n",
      "+----------+---------+-----+----------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      " Job ETL (Raw → Silver) finalizado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "print(\" Validando a tabela fato principal 'lap_times_fact':\\n\")\n",
    "lap_times_fact.printSchema()\n",
    "lap_times_fact.show(10)\n",
    "\n",
    "print(\"\\n Job ETL (Raw → Silver) finalizado com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
