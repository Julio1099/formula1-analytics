{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fa58db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------------------------------+\n",
      "|year|url                                                 |\n",
      "+----+----------------------------------------------------+\n",
      "|2009|http://en.wikipedia.org/wiki/2009_Formula_One_season|\n",
      "|2008|http://en.wikipedia.org/wiki/2008_Formula_One_season|\n",
      "|2007|http://en.wikipedia.org/wiki/2007_Formula_One_season|\n",
      "|2006|http://en.wikipedia.org/wiki/2006_Formula_One_season|\n",
      "|2005|http://en.wikipedia.org/wiki/2005_Formula_One_season|\n",
      "+----+----------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "RAW_DIR    = Path(\"/Users/kalebmacedo/formula1-analytics-1/bronze/dados_originais\")\n",
    "SILVER_DIR = Path(\"./silver\"); SILVER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TABLE = \"seasons.csv\"\n",
    "path = RAW_DIR / TABLE\n",
    "assert path.exists(), f\"Arquivo não encontrado: {path}\"\n",
    "\n",
    "seasons = spark.read.csv(str(path), header=True, inferSchema=True)\n",
    "seasons.show(5, truncate=False)\n",
    "seasons.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d16facb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------------------------------+\n",
      "|year|url                                                 |\n",
      "+----+----------------------------------------------------+\n",
      "|2009|http://en.wikipedia.org/wiki/2009_Formula_One_season|\n",
      "|2008|http://en.wikipedia.org/wiki/2008_Formula_One_season|\n",
      "|2007|http://en.wikipedia.org/wiki/2007_Formula_One_season|\n",
      "+----+----------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) RE-LEITURA do seasons.csv (1.1 de novo, agora na sessão ATUAL)\n",
    "from pathlib import Path\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "RAW_DIR    = Path(\"/Users/kalebmacedo/formula1-analytics-1/bronze/dados_originais\")\n",
    "SILVER_DIR = Path(\"./silver\"); SILVER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TABLE = \"seasons.csv\"\n",
    "path = RAW_DIR / TABLE\n",
    "assert path.exists(), f\"Arquivo não encontrado: {path}\"\n",
    "\n",
    "seasons = spark.read.csv(str(path), header=True, inferSchema=True)\n",
    "seasons.show(3, truncate=False)   # sanity rápido\n",
    "seasons.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3894676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros: 75\n",
      "+------+-------+---+\n",
      "|coluna|missing|pct|\n",
      "+------+-------+---+\n",
      "|url   |0      |0.0|\n",
      "|year  |0      |0.0|\n",
      "+------+-------+---+\n",
      "\n",
      "Duplicatas (linhas idênticas): 0\n",
      "Duplicatas de PK (year): 0\n"
     ]
    }
   ],
   "source": [
    "# 3) Agora sim: 1.2 (nulos e duplicatas) usando o seasons RECÉM-LIDO\n",
    "total = seasons.count()\n",
    "print(\"Total de registros:\", total)\n",
    "\n",
    "miss = [(c, seasons.filter(F.col(c).isNull() | (F.col(c) == \"\\\\N\")).count()) for c in seasons.columns]\n",
    "miss = [(c, m, round(100*m/total,2)) for c,m in miss]\n",
    "spark.createDataFrame(miss, [\"coluna\",\"missing\",\"pct\"]).orderBy(F.desc(\"pct\")).show(truncate=False)\n",
    "\n",
    "dup_rows = seasons.groupBy(seasons.columns).count().filter(\"count > 1\").count()\n",
    "print(\"Duplicatas (linhas idênticas):\", dup_rows)\n",
    "\n",
    "dup_pk = seasons.groupBy(\"year\").count().filter(\"count > 1\").count()\n",
    "print(\"Duplicatas de PK (year):\", dup_pk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81a11844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|min_year|max_year|\n",
      "+--------+--------+\n",
      "|    1950|    2024|\n",
      "+--------+--------+\n",
      "\n",
      "Anos ausentes:\n",
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# intervalo de anos\n",
    "seasons.select(F.min(\"year\").alias(\"min_year\"),\n",
    "               F.max(\"year\").alias(\"max_year\")).show()\n",
    "\n",
    "# anos faltando na sequência\n",
    "miny, maxy = seasons.select(F.min(\"year\"), F.max(\"year\")).first()\n",
    "years = spark.range(miny, maxy + 1).withColumnRenamed(\"id\",\"year\")\n",
    "faltando = years.join(seasons.select(\"year\").distinct(), on=\"year\", how=\"left_anti\")\n",
    "print(\"Anos ausentes:\")\n",
    "faltando.orderBy(\"year\").show(200, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf6ff21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver salva em: silver/seasons_silver.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "clean = seasons\n",
    "\n",
    "# trim strings e cast\n",
    "for c,t in clean.dtypes:\n",
    "    if t == \"string\":\n",
    "        clean = clean.withColumn(c, F.trim(F.col(c)))\n",
    "clean = clean.withColumn(\"year\", F.col(\"year\").cast(\"int\"))\n",
    "\n",
    "# faixa “segura” do dataset\n",
    "clean = clean.filter((F.col(\"year\") >= 1950) & (F.col(\"year\") <= 2025))\n",
    "\n",
    "# garantir 1 por ano (se um dia vier duplicado)\n",
    "w = Window.partitionBy(\"year\").orderBy(F.when(F.col(\"url\").isNull(), 1).otherwise(0))\n",
    "clean = clean.withColumn(\"rn\", F.row_number().over(w)).filter(F.col(\"rn\")==1).drop(\"rn\")\n",
    "\n",
    "out = (SILVER_DIR / \"seasons_silver.csv\").as_posix()\n",
    "clean.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(out)\n",
    "print(\"Silver salva em:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0aaa4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver salva em: /Users/kalebmacedo/formula1-analytics-1/silver/seasons_silver.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# caminho absoluto para sua pasta silver\n",
    "SILVER_DIR = Path(\"/Users/kalebmacedo/formula1-analytics-1/silver\")\n",
    "\n",
    "# garante que a pasta existe\n",
    "SILVER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# saída final em formato \"um único CSV\"\n",
    "out = SILVER_DIR / \"seasons_silver.csv\"\n",
    "\n",
    "# salva consolidado em 1 arquivo\n",
    "(\n",
    "    seasons\n",
    "    .coalesce(1)  # junta tudo em 1 partição\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", True)\n",
    "    .csv(str(out))\n",
    ")\n",
    "\n",
    "print(\"Silver salva em:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e65d96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------------------------------+\n",
      "|year|url                                                 |\n",
      "+----+----------------------------------------------------+\n",
      "|2009|http://en.wikipedia.org/wiki/2009_Formula_One_season|\n",
      "|2008|http://en.wikipedia.org/wiki/2008_Formula_One_season|\n",
      "|2007|http://en.wikipedia.org/wiki/2007_Formula_One_season|\n",
      "|2006|http://en.wikipedia.org/wiki/2006_Formula_One_season|\n",
      "|2005|http://en.wikipedia.org/wiki/2005_Formula_One_season|\n",
      "|2004|http://en.wikipedia.org/wiki/2004_Formula_One_season|\n",
      "|2003|http://en.wikipedia.org/wiki/2003_Formula_One_season|\n",
      "|2002|http://en.wikipedia.org/wiki/2002_Formula_One_season|\n",
      "|2001|http://en.wikipedia.org/wiki/2001_Formula_One_season|\n",
      "|2000|http://en.wikipedia.org/wiki/2000_Formula_One_season|\n",
      "+----+----------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\n",
    "    \"/Users/kalebmacedo/formula1-analytics-1/silver/seasons_silver.csv\",\n",
    "    header=True, inferSchema=True\n",
    ").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dca6153c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV único salvo em: /Users/kalebmacedo/formula1-analytics-1/silver/seasons_silver_flat.csv\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import shutil, os\n",
    "\n",
    "dir_out = Path(\"/Users/kalebmacedo/formula1-analytics-1/silver/seasons_silver.csv\")\n",
    "# pega o part \"de verdade\" (ignora ocultos começando com ponto)\n",
    "parts = [p for p in glob(str(dir_out / \"part-*.csv\")) if not os.path.basename(p).startswith(\".\")]\n",
    "assert parts, \"Nenhum part CSV encontrado.\"\n",
    "src_part = parts[0]\n",
    "\n",
    "dst_file = dir_out.with_suffix(\"\")  # -> /.../silver/seasons_silver\n",
    "dst_file = Path(str(dst_file) + \"_flat.csv\")  # seasons_silver_flat.csv\n",
    "shutil.copy(src_part, dst_file)\n",
    "\n",
    "print(\"CSV único salvo em:\", dst_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
